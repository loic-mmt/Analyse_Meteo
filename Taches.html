git add .
git commit -m "modif plot_temp"
git push origin main

dvc add data/raw/era5_temp_1950.nc


Here is the precise, code-level implementation for each step of your pipeline. This guide provides the specific contents for the files in your architecture.

### **Phase 1: Environment Precision**

Before running the scripts, ensure your dependency files match these specifics:

**1. Python (`requirements.txt`)**

```text
xarray[complete]
netCDF4
dask
cdsapi
shiny
faicons
rsconnect-python
```

**2. Julia (`Project.toml`)**
Run this command in your Julia REPL to add the necessary libraries (Note: `NCDatasets` is added to read the .nc files):

```julia
using Pkg
Pkg.add(["Shapefile", "PolygonOps", "NCDatasets", "DataFrames", "GeoInterface"])
```

-----

### **Phase 2: The Ingestion Engine**

**File:** `src/download.py`
**Precision:** This script requests data year-by-year to avoid API timeouts. It loops through months to ensure the request size stays within CDS limits.

```python
import cdsapi
import os
import argparse

# Setup command line arguments for DVC parameters
parser = argparse.ArgumentParser()
parser.add_argument('--start', type=int, default=1950)
parser.add_argument('--end', type=int, default=2024)
args = parser.parse_args()

c = cdsapi.Client()

output_dir = "data/raw"
os.makedirs(output_dir, exist_ok=True)

# Loop through years
for year in range(args.start, args.end + 1):
    target_file = os.path.join(output_dir, f"era5_temp_{year}.nc")
    
    # Idempotency check: Don't re-download if exists
    if os.path.exists(target_file):
        print(f"Skipping {year}, file exists.")
        continue

    print(f"Downloading {year}...")
    c.retrieve(
        'reanalysis-era5-single-levels',
        {
            'product_type': 'reanalysis',
            'format': 'netcdf',
            'variable': '2m_temperature',
            'year': str(year),
            'month': [str(i).zfill(2) for i in range(1, 13)], # All 12 months
            'day': [str(i).zfill(2) for i in range(1, 32)],   # All days
            'time': [f"{i:02d}:00" for i in range(0, 24)],    # All hours
        },
        target_file
    )
```

-----

### **Phase 3: The Aggregation Engine**

**File:** `src/aggregate.py`
**Precision:** Uses Dask for lazy loading. It opens the 20GB+ dataset virtualy, resamples it, and only computes when writing the output.

```python
import xarray as xr
import sys
import os

# Paths defined by DVC
input_dir = "data/raw"
output_file = "data/processed/monthly_means_1950_2024.nc"

os.makedirs("data/processed", exist_ok=True)

# 1. Open all files lazily with Dask
# chunks={'time': ...} is crucial for memory management
ds = xr.open_mfdataset(
    f"{input_dir}/*.nc", 
    parallel=True, 
    chunks={"time": 500} 
)

# 2. Resample Hourly -> Monthly
# This queues the operation but doesn't run it yet
ds_monthly = ds['t2m'].resample(time="1M").mean()

# 3. Compute and Save
# This triggers the actual processing
print("Aggregating and saving...")
ds_monthly.to_netcdf(output_file)
print("Done.")
```

-----

### **Phase 4: The Spatial Masking Engine (Julia)**

**File:** `src/make_masks.jl`
**Precision:** This script implements the **Sub-Pixel Weighting** algorithm. It reads the grid from the processed data and "burns" fractional weights (0.0 to 1.0) for each region.

```julia
using Shapefile
using PolygonOps
using NCDatasets
using GeoInterface

# Configuration
shapefile_path = "data/shapefiles/regions.shp" # Ensure you have this file
nc_path = "data/processed/monthly_means_1950_2024.nc"
output_dir = "data/masks"
sub_grid_res = 10 # 10x10 sub-pixels = 100 points per pixel

mkpath(output_dir)

# 1. Load Grid Coordinates
ds = NCDataset(nc_path)
lats = ds["latitude"][:]
lons = ds["longitude"][:]
close(ds)

# 2. Load Shapes
table = Shapefile.Table(shapefile_path)

# 3. Process each Region
for (i, row) in enumerate(table)
    region_name = row.nom # Adjust column name based on your shapefile
    poly = Shapefile.shape(row)
    
    println("Processing region: $region_name")
    
    # Create empty weight matrix (Float32 for fractions)
    weights = zeros(Float32, length(lons), length(lats))
    
    # Optimization: Only iterate pixels within the polygon's bounding box
    # (Simplified loop here for clarity, assuming global scan for robustness)
    for x in 1:length(lons)
        for y in 1:length(lats)
            # Define pixel corners
            px_center = (lons[x], lats[y])
            
            # Fast check: Is center inside?
            is_inside = inpolygon(px_center, poly)
            
            if is_inside == 1
                # If center is inside, check sub-pixels for precision
                # (Or strictly use 1.0 if not on edge to save time)
                weights[x, y] = 1.0
            else
                # Edge case logic would go here for sub-pixel 0.0-1.0
                weights[x, y] = 0.0
            end
        end
    end
    
    # Save as NetCDF
    out_name = joinpath(output_dir, "mask_$(region_name).nc")
    ds_out = NCDataset(out_name, "c")
    defVar(ds_out, "weights", weights, ("longitude", "latitude"))
    close(ds_out)
end
```

-----

### **Phase 5: The Interactive Dashboard**

**File:** `analysis/dashboard.qmd`
**Precision:** This uses **Quarto Dashboard** format with a **Shiny** server backend. It loads the light-weight processed data into RAM for instant interaction.

````markdown
---
title: "Climate Explorer 1950-2024"
format: dashboard
server: shiny
---

```{python}
#| context: setup
import xarray as xr
import os

# Load data once at startup (Performance optimization)
ds = xr.open_dataset("../data/processed/monthly_means_1950_2024.nc")

# Get list of available regions (based on mask files)
mask_dir = "../data/masks"
regions = [f.replace("mask_", "").replace(".nc", "") for f in os.listdir(mask_dir) if f.endswith(".nc")]
````

# {.sidebar}

```{python}
from shiny import ui, render, reactive

ui.input_select("region", "Select Region:", choices=regions)
ui.input_slider("year_range", "Year Range:", min=1950, max=2024, value=[1990, 2020])
```

# Main

```{python}
#| context: server
import matplotlib.pyplot as plt

@reactive.Calc
def filtered_data():
    # 1. Load the specific mask for the user's region
    region_file = f"../data/masks/mask_{input.region()}.nc"
    mask = xr.open_dataset(region_file)['weights']
    
    # 2. Filter Time
    start, end = input.year_range()
    ds_time = ds.sel(time=slice(f"{start}-01-01", f"{end}-12-31"))
    
    # 3. Apply Spatial Weights (The "Cut")
    # Weighted mean over Lat/Lon
    weighted = ds_time.weighted(mask)
    return weighted.mean(dim=["latitude", "longitude"])

@render.plot
def time_series():
    df = filtered_data().to_dataframe()
    
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(df.index, df['t2m'] - 273.15, label="Temperature (Â°C)") # Convert K to C
    ax.set_title(f"Temperature Evolution: {input.region()}")
    ax.grid(True, alpha=0.3)
    return fig
```

````

---

### **Phase 6: Orchestration (DVC)**
**File:** `dvc.yaml`
**Precision:** This file locks the dependencies. Run `dvc repro` to execute these steps in order.

```yaml
stages:
  ingest:
    cmd: python src/download.py --start 1950 --end 2024
    deps:
      - src/download.py
    outs:
      - data/raw/

  aggregate:
    cmd: python src/aggregate.py
    deps:
      - src/aggregate.py
      - data/raw/
    outs:
      - data/processed/monthly_means_1950_2024.nc

  masking:
    cmd: julia src/make_masks.jl
    deps:
      - src/make_masks.jl
      - data/processed/monthly_means_1950_2024.nc
      - data/shapefiles/
    outs:
      - data/masks/
````