
my-climate-project/
├── .dvc/                   # DVC internal configuration
├── .github/                # CI/CD workflows (optional)
├── data/
│   ├── raw/                # [DVC Tracked] Hourly .nc files (1950-2024)
│   ├── processed/          # [DVC Tracked] optimized monthly/yearly means
│   ├── shapefiles/         # [DVC Tracked] Raw geometries (Regions, Depts)
│   └── masks/              # [DVC Tracked] Pre-computed boolean NetCDF masks
├── src/                    # Pipeline Scripts
│   ├── download.py         # Fetch data via API (Python)
│   ├── aggregate.py        # Resample hourly -> monthly (Python/Xarray)
│   └── make_masks.jl       # Generate boolean masks from shapes (Julia)
├── analysis/               # Quarto Environment
│   ├── _quarto.yml         # Project configuration
│   ├── spatial_masking.qmd # Static Report: Deep dive methodology (Py+Jl)
│   └── dashboard.qmd       # Interactive App: Shiny Server + OJS UI
├── dvc.yaml                # The Orchestrator: Defines the 3 pipeline stages
├── dvc.lock                # The Snapshot: Exact versions of data used
├── requirements.txt        # Python libs: xarray, shiny, netCDF4, etc.
└── Project.toml            # Julia libs: Shapefile, PolygonOps, NPZ

Based on your project architecture and description, here is a complete and rich work pipeline. This pipeline is designed to be **modular, reproducible (via DVC), and high-performance (using Julia for geometry and Python/Shiny for the interface).**

### **Phase 1: Foundation & Environment Setup**

**Objective:** Create a robust environment that handles 20GB+ of data without crashing Git, while enabling both Python and Julia workflows.

1.  **Repository Initialization**

      * Initialize Git: `git init`
      * Initialize DVC: `dvc init`
      * **Crucial Step:** Configure DVC storage (e.g., local drive, S3, or Google Drive) to handle the large files.
          * `dvc remote add -d myremote <url_to_storage>`
      * Create `.gitignore`: Add `data/`, `.venv/`, `.DS_Store`, and `*.nc`.

2.  **Environment Setup (Dual Language)**

      * **Python (I/O & Interface):** Create a virtual environment (`.venv`). Install requirements:
          * `xarray[complete]`, `netCDF4`, `dask` (for parallel processing), `cdsapi` (for ERA5 download), `shiny` (for the dashboard), `rsconnect-python`.
      * **Julia (Computational Engine):** Initialize the project (`julia --project=.`). Add packages:
          * `Pkg.add(["Shapefile", "PolygonOps", "NPZ", "DataFrames", "NetCDF"])`.

3.  **Asset Acquisition (Shapefiles)**

      * Download official French administrative geometries from the **[gregoiredavid/france-geojson](https://github.com/gregoiredavid/france-geojson)** repository.
      * Place them in `data/shapefiles/` (e.g., `regions.geojson`, `departements.geojson`).
      * **Track with DVC:** `dvc add data/shapefiles/`.

-----

### **Phase 2: The Ingestion Pipeline (Python)**

**Objective:** Download 74 years of hourly data (1950–2024) safely.

  * **Script:** `src/download.py`
  * **Logic:**
      * Do **not** attempt to download 74 years in a single request (it will timeout).
      * Write a loop that iterates **year by year**.
      * Use the `cdsapi` client to fetch data.
      * Check if the file `data/raw/era5_temperature_{YEAR}.nc` exists before downloading (idempotency).
  * **DVC Integration:**
      * Define this stage in `dvc.yaml` as `ingest`.
      * **Dependencies:** `src/download.py`
      * **Outputs:** `data/raw/` (This folder will be tracked by DVC, keeping your Git repo light).

-----

### **Phase 3: The Aggregation Engine (Python)**

**Objective:** Reduce data volume by \~730x (Hourly $\rightarrow$ Monthly) to make the dashboard responsive.

  * **Script:** `src/aggregate.py`
  * **Logic:**
      * Use **Xarray** with **Dask** for lazy loading (crucial for 20GB+ data).
      * Load all raw files: `ds = xr.open_mfdataset('data/raw/*.nc', parallel=True)`.
      * Resample to monthly means: `ds_monthly = ds.resample(time='1M').mean()`.
      * Save the result as a single "optimized" chunked NetCDF: `data/processed/temperature_monthly_1950_2024.nc`.
  * **DVC Integration:**
      * Stage name: `process`.
      * **Dependencies:** `data/raw/`, `src/aggregate.py`.
      * **Outputs:** `data/processed/`.

-----

### **Phase 4: The Spatial Masking Engine (Julia)**

**Objective:** Create high-precision spatial masks using Julia's speed for geometry.

  * **Script:** `src/make_masks.jl`
  * **The "Secret Sauce" (Sub-pixel Weighting):**
      * Instead of a simple binary mask (Inside/Outside), calculate **Fractional Weights**.
      * Read the grid coordinates from the processed NetCDF.
      * Load the Shapefiles (Regions/Departments).
      * For each pixel, subdivide it (e.g., into a 10x10 sub-grid). Calculate how many sub-points fall inside the region polygon.
      * Result: A value between `0.0` (Ocean) and `1.0` (Land), allowing for smooth coastline handling.
      * Save these masks as `data/masks/mask_{region_id}.nc`.
  * **DVC Integration:**
      * Stage name: `masking`.
      * **Dependencies:** `data/processed/`, `data/shapefiles/`, `src/make_masks.jl`.
      * **Outputs:** `data/masks/`.

-----

### **Phase 5: The Interactive Dashboard (Quarto + Shiny)**

**Objective:** Visualize the data without loading 20GB into memory.

  * **File:** `analysis/dashboard.qmd`
  * **Architecture:**
      * **Format:** `format: dashboard` with `server: shiny` (Python backend).
      * **UI (Sidebar):**
          * `input_select("region", ...)`: Select between Regions/Departments.
          * `input_slider("year_range", ...)`: Filter the time period (1950–2024).
          * `input_select("month", ...)`: Filter specific months (e.g., "Only January" to see winter warming).
      * **Server Logic (Python):**
          * Load the **Processed** (Monthly) data (approx. 500MB, fits in RAM).
          * Load the **Mask** corresponding to the selected region on-the-fly.
          * **Fast Math:** `result = ds_monthly * mask_region`.
          * Calculate the spatial average and render the plot using `matplotlib` or `plotly`.

-----

### **Phase 6: Orchestration (DVC)**

**Objective:** Automate the entire workflow.

Create a `dvc.yaml` file to link these steps. This allows you to update the entire pipeline with one command.

```yaml
stages:
  ingest:
    cmd: python src/download.py
    deps:
      - src/download.py
    outs:
      - data/raw/

  aggregate:
    cmd: python src/aggregate.py
    deps:
      - src/aggregate.py
      - data/raw/
    outs:
      - data/processed/

  masking:
    cmd: julia src/make_masks.jl
    deps:
      - src/make_masks.jl
      - data/processed/
      - data/shapefiles/
    outs:
      - data/masks/
```

**How to run:**

  * **Update Data:** `dvc repro` (DVC checks which steps need re-running based on file changes).
  * **Visualizing:** `quarto preview analysis/dashboard.qmd`.

### **Summary of Tools & Roles**

| Component | Tool | Why? |
| :--- | :--- | :--- |
| **Orchestration** | **DVC** | Tracks 20GB+ of data and defines the dependency graph (DAG). |
| **Ingestion** | **Python (cdsapi)** | Standard, reliable API client for Climate Data Store. |
| **Processing** | **Python (Xarray)** | Best-in-class for handling labeled multi-dimensional arrays (NetCDF). |
| **Geometry** | **Julia** | Significantly faster than Python for looping through millions of "Point-in-Polygon" checks. |
| **Dashboard** | **Quarto + Shiny** | Combines the narrative of a report with the interactivity of an app. |

This work pipeline is designed to be **dense and actionable**, guiding you from an empty folder to a deployed, high-performance climate dashboard. It follows the **Hybrid Architecture** (Python + Julia + DVC + Quarto).

### **Phase 1: Foundation & Environment (Day 1)**

**Goal:** Create a robust "skeleton" that handles 20GB+ of data without crashing Git.

1.  **Repository Setup**
      * Initialize Git: `git init`.
      * Create `.gitignore` (Ignore `data/`, `.venv`, `.DS_Store`).
2.  **Environment Management (Dual Languages)**
      * **Python:** Create a `venv` or `conda` env. Install: `xarray`, `netCDF4`, `dask`, `shiny`, `cdsapi`. Freeze to `requirements.txt`.
      * **Julia:** Initialize project: `julia --project=.`. Add packages: `Pkg.add(["Shapefile", "PolygonOps", "NPZ", "DataFrames"])`. This creates `Project.toml`.
3.  **DVC storage "Handshake"**
      * Initialize DVC: `dvc init`.
      * **Crucial:** Configure the remote storage (Google Drive) to accept large pushes.
      * *Command:* `dvc remote add -d myremote gdrive://YOUR_FOLDER_ID`
      * *Optimization:* Set `dvc remote modify myremote jobs 4` (Parallel uploads).

### **Phase 2: The Ingestion Engine (Days 2-3)**

**Goal:** Download 74 years of hourly data without hitting API limits or memory overflows.

1.  **Develop `src/download.py`**
      * **Logic:** Do not download 1 file. Write a loop that downloads **1 file per year** (e.g., `era5_1950.nc`).
      * **API:** Use `cdsapi` (Copernicus).
      * **Safety:** Add a `try/except` block to retry failed downloads automatically.
2.  **DVC Integration (Stage 1)**
      * Do **not** run the python script manually. Define it in `dvc.yaml`.
      * *Stage Name:* `download_data`
      * *Command:* `python src/download.py`
      * *Output:* `data/raw/`

### **Phase 3: The Aggregation "Crusher" (Day 4)**

**Goal:** Reduce data volume by 700x (Hourly $\rightarrow$ Monthly) to make the dashboard instant.

1.  **Develop `src/aggregate.py`**
      * **Tool:** Python + Xarray + Dask (for parallel processing).
      * **Logic:**
          * Open all raw files: `xr.open_mfdataset("data/raw/*.nc")`.
          * Resample: `ds.resample(time="1M").mean()`.
          * **Save:** Export as `data/processed/all_years_monthly.nc`.
2.  **DVC Integration (Stage 2)**
      * *Stage Name:* `aggregate`
      * *Dependency:* `data/raw/` (If raw data changes, this re-runs automatically).
      * *Output:* `data/processed/`

### **Phase 4: The Julia Geometry Engine (Days 5-6)**

**Goal:** Solve the "Pixel Cutting" problem. Create precise **Fractional Weights** for every region.

1.  **Data Prep:**
      * Download standard Shapefiles (Regions, Departments) into `data/shapefiles/`.
2.  **Develop `src/make_masks.jl`**
      * **Input:** Reads the lat/lon coordinates from `data/processed/all_years_monthly.nc`.
      * **The Algorithm:** Implement the **Sub-Pixel Sampling** logic (divide each pixel into 10x10 sub-points).
      * **Output:** Generates a NetCDF file for *each* region (e.g., `mask_bretagne.nc`) containing values $0.0 \to 1.0$.
3.  **DVC Integration (Stage 3)**
      * *Stage Name:* `compute_masks`
      * *Dependency:* `data/shapefiles/` and `src/make_masks.jl`.
      * *Output:* `data/masks/`

### **Phase 5: The "Deep Dive" Report (Day 7)**

**Goal:** Validate the science before building the app.

1.  **Create `analysis/spatial_masking.qmd`**
      * **Header:** `format: html`, `engine: jupyter`.
      * **Content:**
          * Load a raw file (Python).
          * Load a fractional mask generated by Julia.
          * **Visual Validation:** Plot the mask. *Does the coastline look pixelated or smooth?* (It should look smooth/grey-scaled at the edges).
      * **Render:** Run `quarto render` to create a permanent HTML proof of your methodology.

### **Phase 6: The Interactive Dashboard (Days 8-9)**

**Goal:** Build the tool that lets users explore the data live.

1.  **Create `analysis/dashboard.qmd`**
      * **Header:** `format: dashboard`, `server: shiny`.
2.  **Frontend (OJS):**
      * Create the "Sidebar": Dropdown for `Region`, Slider for `Year Range`.
      * *Key:* These inputs send signals to the Python backend.
3.  **Backend (Python/Shiny):**
      * **Load Once:** Load `data/processed/all_years_monthly.nc` into RAM (It's small now\!).
      * **Reactive Calc:**
        ```python
        @reactive.Calc
        def filtered_data():
             # 1. Load the specific mask for the selected region
             mask = xr.open_dataset(f"data/masks/{input.region()}.nc")
             # 2. Apply Weighted Mean
             return ds.weighted(mask).mean(...)
        ```
      * **Plot:** Render the time-series graph using `matplotlib` or `plotly`.

### **Phase 7: Optimization & Automation (Day 10)**

**Goal:** Lock it down.

1.  **Full Pipeline Test**
      * Run `dvc repro`.
      * *Result:* DVC checks if `download` needs to run (No) $\rightarrow$ checks `aggregate` (No) $\rightarrow$ checks `masks` (No). Everything is cached.
2.  **Cloud Sync**
      * Run `dvc push`.
      * *Result:* Your 20GB of raw data, processed means, and masks are sent to Google Drive.
3.  **Git Commit**
      * Commit the `dvc.lock` file. This is your "Save State." Any other developer can clone this repo, run `dvc pull`, and have your exact environment.

### **Summary of the "Dense" Pipeline**

| Day | Task | Key Tech | Output |
| :--- | :--- | :--- | :--- |
| **1** | **Setup** | Git, DVC, Conda, Pkg | Empty repo, envs ready |
| **2-3** | **Ingestion** | Python `cdsapi` | `data/raw/*.nc` (20GB) |
| **4** | **Aggregation** | Xarray + Dask | `data/processed/monthly.nc` (500MB) |
| **5-6** | **Geometry** | **Julia** (Sub-pixel) | `data/masks/*.nc` (Weights) |
| **7** | **Validation** | Quarto Report | `spatial_masking.html` |
| **8-9** | **Dashboard** | OJS + Shiny | `dashboard.qmd` (Live App) |
| **10** | **Freeze** | DVC Repro/Push | `dvc.lock` (Reproducible Science) |