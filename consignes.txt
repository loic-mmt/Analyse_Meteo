This work pipeline is designed to be **dense and actionable**, guiding you from an empty folder to a deployed, high-performance climate dashboard. It follows the **Hybrid Architecture** (Python + Julia + DVC + Quarto).

### **Phase 1: Foundation & Environment (Day 1)**

**Goal:** Create a robust "skeleton" that handles 20GB+ of data without crashing Git.

1.  **Repository Setup**
      * Initialize Git: `git init`.
      * Create `.gitignore` (Ignore `data/`, `.venv`, `.DS_Store`).
2.  **Environment Management (Dual Languages)**
      * **Python:** Create a `venv` or `conda` env. Install: `xarray`, `netCDF4`, `dask`, `shiny`, `cdsapi`. Freeze to `requirements.txt`.
      * **Julia:** Initialize project: `julia --project=.`. Add packages: `Pkg.add(["Shapefile", "PolygonOps", "NPZ", "DataFrames"])`. This creates `Project.toml`.
3.  **DVC storage "Handshake"**
      * Initialize DVC: `dvc init`.
      * **Crucial:** Configure the remote storage (Google Drive) to accept large pushes.
      * *Command:* `dvc remote add -d myremote gdrive://YOUR_FOLDER_ID`
      * *Optimization:* Set `dvc remote modify myremote jobs 4` (Parallel uploads).

### **Phase 2: The Ingestion Engine (Days 2-3)**

**Goal:** Download 74 years of hourly data without hitting API limits or memory overflows.

1.  **Develop `src/download.py`**
      * **Logic:** Do not download 1 file. Write a loop that downloads **1 file per year** (e.g., `era5_1950.nc`).
      * **API:** Use `cdsapi` (Copernicus).
      * **Safety:** Add a `try/except` block to retry failed downloads automatically.
2.  **DVC Integration (Stage 1)**
      * Do **not** run the python script manually. Define it in `dvc.yaml`.
      * *Stage Name:* `download_data`
      * *Command:* `python src/download.py`
      * *Output:* `data/raw/`

### **Phase 3: The Aggregation "Crusher" (Day 4)**

**Goal:** Reduce data volume by 700x (Hourly $\rightarrow$ Monthly) to make the dashboard instant.

1.  **Develop `src/aggregate.py`**
      * **Tool:** Python + Xarray + Dask (for parallel processing).
      * **Logic:**
          * Open all raw files: `xr.open_mfdataset("data/raw/*.nc")`.
          * Resample: `ds.resample(time="1M").mean()`.
          * **Save:** Export as `data/processed/all_years_monthly.nc`.
2.  **DVC Integration (Stage 2)**
      * *Stage Name:* `aggregate`
      * *Dependency:* `data/raw/` (If raw data changes, this re-runs automatically).
      * *Output:* `data/processed/`

### **Phase 4: The Julia Geometry Engine (Days 5-6)**

**Goal:** Solve the "Pixel Cutting" problem. Create precise **Fractional Weights** for every region.

1.  **Data Prep:**
      * Download standard Shapefiles (Regions, Departments) into `data/shapefiles/`.
2.  **Develop `src/make_masks.jl`**
      * **Input:** Reads the lat/lon coordinates from `data/processed/all_years_monthly.nc`.
      * **The Algorithm:** Implement the **Sub-Pixel Sampling** logic (divide each pixel into 10x10 sub-points).
      * **Output:** Generates a NetCDF file for *each* region (e.g., `mask_bretagne.nc`) containing values $0.0 \to 1.0$.
3.  **DVC Integration (Stage 3)**
      * *Stage Name:* `compute_masks`
      * *Dependency:* `data/shapefiles/` and `src/make_masks.jl`.
      * *Output:* `data/masks/`

### **Phase 5: The "Deep Dive" Report (Day 7)**

**Goal:** Validate the science before building the app.

1.  **Create `analysis/spatial_masking.qmd`**
      * **Header:** `format: html`, `engine: jupyter`.
      * **Content:**
          * Load a raw file (Python).
          * Load a fractional mask generated by Julia.
          * **Visual Validation:** Plot the mask. *Does the coastline look pixelated or smooth?* (It should look smooth/grey-scaled at the edges).
      * **Render:** Run `quarto render` to create a permanent HTML proof of your methodology.

### **Phase 6: The Interactive Dashboard (Days 8-9)**

**Goal:** Build the tool that lets users explore the data live.

1.  **Create `analysis/dashboard.qmd`**
      * **Header:** `format: dashboard`, `server: shiny`.
2.  **Frontend (OJS):**
      * Create the "Sidebar": Dropdown for `Region`, Slider for `Year Range`.
      * *Key:* These inputs send signals to the Python backend.
3.  **Backend (Python/Shiny):**
      * **Load Once:** Load `data/processed/all_years_monthly.nc` into RAM (It's small now\!).
      * **Reactive Calc:**
        ```python
        @reactive.Calc
        def filtered_data():
             # 1. Load the specific mask for the selected region
             mask = xr.open_dataset(f"data/masks/{input.region()}.nc")
             # 2. Apply Weighted Mean
             return ds.weighted(mask).mean(...)
        ```
      * **Plot:** Render the time-series graph using `matplotlib` or `plotly`.

### **Phase 7: Optimization & Automation (Day 10)**

**Goal:** Lock it down.

1.  **Full Pipeline Test**
      * Run `dvc repro`.
      * *Result:* DVC checks if `download` needs to run (No) $\rightarrow$ checks `aggregate` (No) $\rightarrow$ checks `masks` (No). Everything is cached.
2.  **Cloud Sync**
      * Run `dvc push`.
      * *Result:* Your 20GB of raw data, processed means, and masks are sent to Google Drive.
3.  **Git Commit**
      * Commit the `dvc.lock` file. This is your "Save State." Any other developer can clone this repo, run `dvc pull`, and have your exact environment.

### **Summary of the "Dense" Pipeline**

| Day | Task | Key Tech | Output |
| :--- | :--- | :--- | :--- |
| **1** | **Setup** | Git, DVC, Conda, Pkg | Empty repo, envs ready |
| **2-3** | **Ingestion** | Python `cdsapi` | `data/raw/*.nc` (20GB) |
| **4** | **Aggregation** | Xarray + Dask | `data/processed/monthly.nc` (500MB) |
| **5-6** | **Geometry** | **Julia** (Sub-pixel) | `data/masks/*.nc` (Weights) |
| **7** | **Validation** | Quarto Report | `spatial_masking.html` |
| **8-9** | **Dashboard** | OJS + Shiny | `dashboard.qmd` (Live App) |
| **10** | **Freeze** | DVC Repro/Push | `dvc.lock` (Reproducible Science) |